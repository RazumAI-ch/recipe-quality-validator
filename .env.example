# =====================================================
# ⚠️ WARNING:
#   This is an example environment configuration file.
#   Do NOT use this file directly in production.
#
#   Only one LLM_BACKEND should be active at runtime.
#   Copy the appropriate section (OPENAI or INTERNAL)
#   into your actual .env file (e.g. `.env.openai` or `.env.internal`)
#   and uncomment only the relevant lines.
# =====================================================

# =====================================================
# === OPENAI BACKEND (api.openai.com) ================
# =====================================================

# Uncomment to use OpenAI as the backend
# LLM_BACKEND=OPENAI

# Your OpenAI API key (only used when LLM_BACKEND=OPENAI)
# OPENAI_API_KEY=sk-...

# Maximum number of recipe entries to validate per run
# MAX_ENTRIES=20


# =====================================================
# === PORTKEY BACKEND (e.g. Azure OpenAI) ============
# =====================================================

# Uncomment to use Portkey to route to Azure OpenAI
# LLM_BACKEND=INTERNAL

# Portkey API key (used to authenticate with Portkey's API gateway)
# INTERNAL_API_PORT_KEY=your-portkey-api-key

# The Portkey target URL (usually an Azure-compatible OpenAI endpoint)
# INTERNAL_API_URL=https://oai-gal-prd-use-001-aihub-007.galileo.roche.com/openai

# Portkey virtual key for selecting routing config
# PORTKEY_VIRTUAL_KEY=default-azure-v-72fa7e

# Name of the LLM provider (e.g. azure-openai, vertex-ai, etc.)
# PORTKEY_PROVIDER=azure-openai

# Number of retry attempts Portkey should use
# PORTKEY_RETRY_ATTEMPTS=5

# Maximum number of recipe entries to validate per run
# MAX_ENTRIES=100